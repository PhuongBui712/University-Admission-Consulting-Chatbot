{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c160cc63a54e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.464719Z",
     "start_time": "2024-05-26T09:20:24.035080Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from hashlib import sha256\n",
    "from urllib.parse import urlparse, quote\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from src.prompts import IMAGE_EXTRACTOR_PROMPT\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a991871e46c5c",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c895d6edafb99",
   "metadata": {},
   "source": [
    "### Image extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718b748cc4024ee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.467261Z",
     "start_time": "2024-05-26T09:20:24.465583Z"
    }
   },
   "outputs": [],
   "source": [
    "class InvalidAPIKey(Exception):\n",
    "    message = 'Invalid API Key'\n",
    "    def __init__(self):\n",
    "        super().__init__(self.message)\n",
    "        \n",
    "\n",
    "class InvalidURL(Exception):\n",
    "    message = 'Invalid URL'\n",
    "    def __init__(self):\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9ec2fd249c71b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.470068Z",
     "start_time": "2024-05-26T09:20:24.467816Z"
    }
   },
   "outputs": [],
   "source": [
    "class GeminiImageExtractor:\n",
    "    prompt = IMAGE_EXTRACTOR_PROMPT\n",
    "    model_list = ['gemini-1.5-flash-latest',\n",
    "                  'gemini-1.5-pro-latest']\n",
    "    def __init__(self, model_name='gemini-1.5-pro-latest'):\n",
    "        self.api = self._get_api_()\n",
    "        self.chat_model = ChatGoogleGenerativeAI(model=model_name, google_api_key=self.api)\n",
    "        self.text_content = {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": self.prompt\n",
    "        }\n",
    "        \n",
    "    def extract(self, image_path, sleep_time=0):\n",
    "        image_content = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": image_path\n",
    "        }\n",
    "        message = HumanMessage(content=[self.text_content, image_content])\n",
    "        \n",
    "        result = self.chat_model.invoke([message])\n",
    "        \n",
    "        sleep(sleep_time)\n",
    "        return result.content\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_api_(self):\n",
    "        key = os.getenv('GOOGLE_API_KEY')\n",
    "        if not key:\n",
    "            raise InvalidAPIKey\n",
    "        \n",
    "        return key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0eb381c84d373",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f08fe06f03df21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.472308Z",
     "start_time": "2024-05-26T09:20:24.470559Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_json(path, items):\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(items, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3579c00654d157",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.474708Z",
     "start_time": "2024-05-26T09:20:24.473479Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_subdirectory(href):\n",
    "    parsed = urlparse(href)\n",
    "    # Check if the scheme and netloc are present\n",
    "    return not (bool(parsed.scheme) and bool(parsed.netloc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f3a3cbdd4cac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.476555Z",
     "start_time": "2024-05-26T09:20:24.475148Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_attachment(url):\n",
    "    # TODO: generalize to various file types\n",
    "    return url.endswith('.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10184c2dea40def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web_soup(url):\n",
    "    response = requests.get(url, headers=REQUEST_HEADER)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577fb97d7741617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_soup(soup):\n",
    "    # get the main content\n",
    "    main_content = soup.find('section', {'id': 'sp-main-body'})\n",
    "\n",
    "    # decompose navigate elements (next or previous page navigators)\n",
    "    navigator_element = main_content.find('ul', {'class': 'pager pagenav'})\n",
    "    if navigator_element:\n",
    "        navigator_element.decompose()\n",
    "\n",
    "    # decompose hit numbers\n",
    "    for element in soup.find_all('span', {'class': 'mod-articles-category-hits'}):\n",
    "        element.decompose()\n",
    "\n",
    "    return main_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af7037e556bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def website_is_updated(url, hash_value):\n",
    "    soup = get_web_soup(url)\n",
    "    if soup:\n",
    "        main_soup = preprocess_soup(soup)\n",
    "        return sha256(main_soup.encode()).hexdigest() != hash_value\n",
    "\n",
    "    raise Exception('Can not connect to destination URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d715e2eeed56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_url(url):\n",
    "    if is_subdirectory(url):\n",
    "        raise InvalidURL\n",
    "\n",
    "    parsed_url = urlparse(url)\n",
    "    subdirectory = parsed_url.path\n",
    "    encoded_subdirectory = quote(subdirectory)\n",
    "    encoded_url = f'{parsed_url.scheme}://{parsed_url.netloc}{encoded_subdirectory}'\n",
    "\n",
    "    return encoded_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8d5bea5a4095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    title = soup.find('meta', {'property': 'og:title'}).get('content') or soup.title.get_text()\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d1f0d231bd1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(soup, internal_link=True, external_link=False, attachment=True, start_url=None):\n",
    "    # get links\n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if attachment and is_attachment_file(a['href']):\n",
    "            links.add(a['href'])\n",
    "\n",
    "        else:\n",
    "            is_internal_link = is_subdirectory(a['href'])\n",
    "            if is_internal_link and internal_link:\n",
    "                links.add(encode_url(start_url + a['href']))\n",
    "            elif not is_internal_link and external_link:\n",
    "                links.add(encode_url(a['href']))\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355f41e89a2c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(url):\n",
    "    response = requests.get(url, headers=REQUEST_HEADER)\n",
    "    if not response.ok:\n",
    "        response.raise_for_status()\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e23e7c958cd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_table(table_soup):\n",
    "    caption = table_soup.find('caption')\n",
    "    if caption is None or not caption.get_text().startswith('Attachments'):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a56b469cbd01844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(soup):\n",
    "    table_elements = []\n",
    "    for table in soup.find_all('table'):\n",
    "        if is_table(table):\n",
    "            table_elements.append(str(table))\n",
    "            table.decompose()\n",
    "\n",
    "    return table_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc1295ba16e673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_website_image(soup, extractor, start_url=None):\n",
    "    not_parsed_imgs = []\n",
    "    for img in soup.find_all('img', src=True):\n",
    "        if img['src'].split('.')[-1] == 'gif':\n",
    "            continue\n",
    "\n",
    "        url = img['src']\n",
    "        if is_subdirectory(url):\n",
    "            try:\n",
    "                url = start_url + url\n",
    "            except:\n",
    "                raise Exception(\"Can not access incomplete URL\")\n",
    "\n",
    "        parse_content = extractor.invoke(url)\n",
    "        if not parse_content.startswith('others'):\n",
    "            img.insert_after(parse_content)\n",
    "        elif not parse_content.startswith('nothing'):\n",
    "            not_parsed_imgs.append(url)\n",
    "\n",
    "    return soup, not_parsed_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639001e876c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_website_url(soup, start_url):\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if a.string and a.string.strip() != a['href']:\n",
    "            original_url = start_url + a['href'] if is_subdirectory(a['href']) else a['href']\n",
    "            a.string += f' ({original_url})'\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e46d4780b66628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_website(soup, parse_reference=True, parse_image=False, start_url=None):\n",
    "    # kill all script and style elements\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.decompose()\n",
    "\n",
    "    # parse image\n",
    "    imgs = []\n",
    "    if parse_image:\n",
    "        image_extractor = GeminiImageExtractor()\n",
    "        soup, imgs = parse_website_image(soup, image_extractor, start_url=start_url)\n",
    "\n",
    "    # parse references\n",
    "    if parse_reference:\n",
    "        soup = parse_website_url(soup, start_url)\n",
    "\n",
    "    tables = get_table(soup)\n",
    "\n",
    "    # parse content\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split('  '))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return text, tables, imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774ee218e665152",
   "metadata": {},
   "source": [
    "## Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff8361b043ddd",
   "metadata": {},
   "source": [
    "**rule:** There is not any 2 articles have different release year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e795dda619f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl functions\n",
    "def crawl_webpage(url, parse_reference=True, parse_image=False, hash=False):\n",
    "    reference_urls = set()\n",
    "    page_content = None\n",
    "    hash_value = None\n",
    "\n",
    "    soup = get_web_soup(url)\n",
    "    if soup:\n",
    "        title = get_title(soup)\n",
    "        main_soup = preprocess_soup(soup)\n",
    "\n",
    "        # hash\n",
    "        if hash:\n",
    "            hash_value = sha256(main_soup.encode()).hexdigest()\n",
    "\n",
    "        # extract start url\n",
    "        parsed_url = urlparse(url)\n",
    "        start_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "\n",
    "        # get references\n",
    "        reference_urls.update(get_links(main_soup,\n",
    "                                              internal_link=True,\n",
    "                                              external_link=False,\n",
    "                                              attachment=True,\n",
    "                                              start_url=start_url))\n",
    "\n",
    "        # parse content\n",
    "        text, tables, images = parse_website(main_soup, parse_reference, parse_image, start_url)\n",
    "\n",
    "        ret = title, text, tables, images, reference_urls\n",
    "        if hash:\n",
    "            ret += (hash_value,)\n",
    "\n",
    "        return ret\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd763a9ef37b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(storage_urls: dict):\n",
    "    # essential variables\n",
    "    start_url = 'https://tuyensinh.hcmus.edu.vn'\n",
    "    news_url = 'https://tuyensinh.hcmus.edu.vn/th%C3%B4ng-tin-tuy%E1%BB%83n-sinh-%C4%91%E1%BA%A1i-h%E1%BB%8Dc'\n",
    "    need2crawl_url = set(storage_urls['base_url']).difference(news_url)\n",
    "    crawled_url = set()\n",
    "    data_dict = {}\n",
    "\n",
    "    # crawl\n",
    "    soup = get_web_soup(news_url)\n",
    "    if soup:\n",
    "        main_soup = preprocess_soup(soup)\n",
    "\n",
    "        # check latest article\n",
    "        latest_article_tag = main_soup.find('li')\n",
    "        latest_article_url = latest_article_tag.find('a')['href']\n",
    "        latest_article_url = start_url + latest_article_url if is_subdirectory(latest_article_url) else latest_article_url\n",
    "\n",
    "\n",
    "        if latest_article_url not in storage_urls['article_url']:\n",
    "            latest_article_release_date = latest_article_tag.find('span', class_=\"mod-articles-category-date\").text.strip()\n",
    "            random_crawled_url = None\n",
    "            valid_year = datetime.now().year\n",
    "\n",
    "            if storage_urls['article_url']:\n",
    "                random_crawled_url = list(storage_urls['article_url'].items())[0]\n",
    "\n",
    "            # case 2: article is of new year\n",
    "            if random_crawled_url and random_crawled_url[1]['release_date'][-4:] == latest_article_release_date[-4:]:\n",
    "                # delete old crawled article\n",
    "                valid_year = random_crawled_url[1]['release_date'][-4:]\n",
    "\n",
    "            # case 3: update of this year\n",
    "            else:\n",
    "                storage_urls['article_url'].clear()\n",
    "\n",
    "            # get new articles\n",
    "            new_article_urls = set()\n",
    "            for tag in main_soup.find_all('li'):\n",
    "                release_date = tag.find('span', class_=\"mod-articles-category-date\").text.strip()\n",
    "                url = tag.find('a', class_=\"mod-articles-category-title\")['href']\n",
    "                if url and is_subdirectory(url):\n",
    "                    url = start_url + url\n",
    "                if url not in storage_urls['article_url'] and int(release_date[-4:]) == valid_year:\n",
    "                    new_article_urls.add((encode_url(url), release_date))\n",
    "\n",
    "            need2crawl_url.update(new_article_urls)\n",
    "\n",
    "            # crawl new article\n",
    "            for url, release_date in new_article_urls:\n",
    "                crawled_url.add(url)\n",
    "\n",
    "                title, text, tables, image_paths, references, hash_value = crawl_webpage(url, parse_reference=True, parse_image=True, hash=True)\n",
    "                storage_urls['article_url'][url] = {\n",
    "                    'release_date': release_date,\n",
    "                    'hash_value': hash_value,\n",
    "                }\n",
    "\n",
    "                data_dict[url] = {}\n",
    "                data_dict[url]['text'] = text\n",
    "                data_dict[url]['table'] = tables\n",
    "                data_dict[url]['image_path'] = image_paths\n",
    "                data_dict[url]['title'] = title\n",
    "\n",
    "                need2crawl_url.update(references)\n",
    "\n",
    "        # crawl base url\n",
    "        for url in storage_urls['base_url'].keys():\n",
    "            hash_value = storage_urls['base_url'][url].get('hash_value')\n",
    "            if website_is_updated(url, hash_value):\n",
    "                title, text, tables, image_paths, references, hash_value = crawl_webpage(url, parse_reference=True, parse_image=True, hash=True)\n",
    "                storage_urls['base_url'][url] = {\n",
    "                    'hash_value': hash_value,\n",
    "                }\n",
    "\n",
    "                data_dict[url] = {}\n",
    "                data_dict[url]['text'] = text\n",
    "                data_dict[url]['table'] = tables\n",
    "                data_dict[url]['image_path'] = image_paths\n",
    "                data_dict[url]['title'] = title\n",
    "\n",
    "                need2crawl_url.update(references)\n",
    "\n",
    "        crawled_url.update(storage_urls['base_url'].keys())\n",
    "\n",
    "        # check if complete all references\n",
    "        # TODO: Handle remaining URLs\n",
    "        need2crawl_url = need2crawl_url.difference(crawled_url)\n",
    "\n",
    "        return storage_urls, data_dict\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc022496418374a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:36:19.354021Z",
     "start_time": "2024-05-26T09:20:24.504492Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data/sitemap.json', 'r', encoding='utf-8') as file:\n",
    "    sitemap = json.load(file)\n",
    "    \n",
    "updated_sitemap, data = crawl(sitemap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
