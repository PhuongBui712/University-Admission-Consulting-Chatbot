{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T20:23:21.768344Z",
     "start_time": "2024-06-15T20:23:21.747902Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import base64\n",
    "from hashlib import sha256\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Union\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAI\n",
    "from langchain_cohere import CohereRerank, CohereEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "\n",
    "from src.scraper.scrape import crawl, crawl_webpage\n",
    "from src.prompts import TEXT_SUMMARIZE_PROMPT, IMAGE_SUMMARIZE_PROMPT\n",
    "from src.image_extractor import (\n",
    "    GeminiImageExtractor,\n",
    "    get_image,\n",
    "    encode_image\n",
    ")\n",
    "from src.agent_chunking import GeminiChunker\n",
    "from src.embedding import GeminiEmbedding\n",
    "from src.utils import load_json, write_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T12:32:08.966802Z",
     "start_time": "2024-06-15T12:32:08.962992Z"
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the crawling data code, uncomment and run it if this is the first time running code. Or check the `data/tmp_docs.json` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T12:32:08.969083Z",
     "start_time": "2024-06-15T12:32:08.967480Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_dict = crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(os.getcwd(), 'data/tmp_data.json'), 'w', encoding='utf-8') as f:\n",
    "#     json.dump(data_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert raw text to Document\n",
    "# texts = [Document(page_content=v['text'], metadata={'source': {'url': k, 'title': v['title']}}) for k, v in data_dict.items()]\n",
    "\n",
    "# # convert raw table to Document\n",
    "# tables = []\n",
    "# for k, v in data_dict.items():\n",
    "#     for table in v['table']:\n",
    "#         tables.append(Document(page_content=table, metadata={'source': {'url': k, 'title': v['title']}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the available crawled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = load_json(os.path.join(os.getcwd(), 'data/tmp_data.json'))\n",
    "\n",
    "texts = [Document(page_content=v['text'], metadata={'source': {'url': k, 'title': v['title']}}) for k, v in data_dict.items()]\n",
    "\n",
    "tables = []\n",
    "for k, v in data_dict.items():\n",
    "    for table in v['table']:\n",
    "        tables.append(Document(page_content=table, metadata={'source': {'url': k, 'title': v['title']}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunker = GeminiChunker()\n",
    "# chunks = chunker.split_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize process\n",
    "\n",
    "Uncomment to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = 'gemini-1.5-flash-latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text_summaries(texts: Union[List[str], List[Document]], tables: Union[List[str], List[Document]], summary_text: bool = True):\n",
    "#     prompt = PromptTemplate.from_template(template=TEXT_SUMMARIZE_PROMPT)\n",
    "#     empty_response = RunnableLambda(lambda x : AIMessage(content=\"Error processing document\"))\n",
    "#     model = GoogleGenerativeAI(model=MODEL_NAME, temperature=0.0, top_k=1, top_p=0.1).with_fallbacks([empty_response])\n",
    "#     chain = prompt | model | StrOutputParser()\n",
    "\n",
    "#     text_summaries = []\n",
    "#     table_summaries = []\n",
    "#     if texts and summary_text:\n",
    "#         if isinstance(texts[0], Document):\n",
    "#             texts = [t.page_content for t in texts]\n",
    "        \n",
    "#         text_summaries = chain.batch(texts, {'max_concurrency':1})\n",
    "#     else:\n",
    "#         text_summaries = texts\n",
    "\n",
    "#     if tables:\n",
    "#         if isinstance(tables[0], Document):\n",
    "#             tables = [t.page_content for t in tables]\n",
    "#         table_summaries = chain.batch(tables, {'max_concurrency':1})\n",
    "\n",
    "#     return text_summaries, table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_summaries, table_summaries = generate_text_summaries(texts=texts, tables=tables, summary_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_image_summaries(image_paths):\n",
    "#     model = GeminiImageExtractor(custom_prompt=IMAGE_SUMMARIZE_PROMPT)\n",
    "#     image_summaries = []\n",
    "#     b64_images = []\n",
    "#     for image in tqdm(sorted(image_paths)):\n",
    "#         image_summaries.append(model.invoke(image))\n",
    "#         b64_images.append(encode_image(get_image(image)))\n",
    "\n",
    "#     return b64_images, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = []\n",
    "# image_summaries = []\n",
    "# for k, v in data_dict.items():\n",
    "#     if v['image_path']:\n",
    "#         b64_images, summaries = generate_image_summaries(v['image_path'])\n",
    "#         images += [Document(page_content=image, metadata={'source': {'url': k, 'title': v['title']}}) for image in b64_images]\n",
    "#         image_summaries += summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved_docs = {\n",
    "#     'texts': [json.loads(text.json(ensure_ascii=False)) for text in texts],\n",
    "#     'tables': [json.loads(table.json(ensure_ascii=False)) for table in tables],\n",
    "#     'images': [json.loads(image.json(ensure_ascii=False)) for image in images],\n",
    "# }\n",
    "\n",
    "# saved_summaries = {\n",
    "#     'text_summaries': text_summaries,\n",
    "#     'table_summaries': table_summaries,\n",
    "#     'image_summaries': image_summaries\n",
    "# }\n",
    "\n",
    "# write_json(os.path.join(os.getcwd(), 'data/documents.json'), saved_docs)\n",
    "# write_json(os.path.join(os.getcwd(), 'data/summaries.json'), saved_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use saved summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_json(os.path.join(os.getcwd(), 'data/documents.json'))\n",
    "summaries = load_json(os.path.join(os.getcwd(), 'data/summaries.json'))\n",
    "\n",
    "texts = [Document(**t) for t in documents['texts']]\n",
    "tables = [Document(**t) for t in documents['tables']]\n",
    "images = [Document(**i) for i in documents['images']]\n",
    "\n",
    "text_summaries, table_summaries, image_summaries = summaries['text_summaries'], summaries['table_summaries'], summaries['image_summaries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Multi-vector Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = 'multimodal'\n",
    "namespace = f'chroma/{collection_name}'\n",
    "db_path = os.path.join(os.getcwd(), 'record_manager_cache.sql')\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace=namespace,\n",
    "    db_url=f'sqlite:///{os.path.join(os.getcwd(), 'database/record_manager_cache.sql')}'\n",
    ")\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    record_manager.create_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name='test',\n",
    "    collection_metadata={'hnsw:space': 'cosine'},\n",
    "    embedding_function=CohereEmbeddings(model='embed-multilingual-v3.0'),\n",
    "    persist_directory=os.path.join(os.getcwd(), 'database')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = LocalFileStore(root_path=os.path.join(os.getcwd(), 'database/docstore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_contents = [t.page_content.encode() for t in texts] + [t.page_content.encode() for t in tables] + [i.page_content.encode() for i in images]\n",
    "\n",
    "id_key = 'doc_id'\n",
    "id_map = {}\n",
    "doc_ids = []\n",
    "for doc in (texts + tables + images):\n",
    "    id = str(uuid.uuid4())\n",
    "\n",
    "    hash_source = sha256(doc.metadata['source']['url'].encode('utf-8')).hexdigest()\n",
    "    if hash_source not in id_map:\n",
    "        id_map[hash_source] = doc.metadata['source']\n",
    "        id_map[hash_source]['doc_ids'] = []\n",
    "    id_map[hash_source]['doc_ids'].append(id)\n",
    "    \n",
    "    doc_ids.append(id)\n",
    "\n",
    "doc_summaries = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]}) \\\n",
    "    for i, s in enumerate(text_summaries + table_summaries + image_summaries)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index(\n",
    "    docs_source=doc_summaries,\n",
    "    record_manager=record_manager,\n",
    "    vector_store=vectorstore,\n",
    "    cleanup='full',\n",
    "    source_id_key=id_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.mset(list(zip(doc_ids, doc_contents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(os.path.join(os.getcwd(), 'database/id_map.json'), id_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    "    search_type='similarity',\n",
    "    search_kwargs={'k':15}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
