{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c160cc63a54e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.464719Z",
     "start_time": "2024-05-26T09:20:24.035080Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from hashlib import sha256\n",
    "from urllib.parse import urlparse, quote\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from src.prompts import IMAGE_EXTRACTOR_PROMPT\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2a991871e46c5c",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718b748cc4024ee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.467261Z",
     "start_time": "2024-05-26T09:20:24.465583Z"
    }
   },
   "outputs": [],
   "source": [
    "class InvalidAPIKey(Exception):\n",
    "    message = 'Invalid API Key'\n",
    "    def __init__(self):\n",
    "        super().__init__(self.message)\n",
    "        \n",
    "\n",
    "class InvalidURL(Exception):\n",
    "    message = 'Invalid URL'\n",
    "    def __init__(self):\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9ec2fd249c71b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.470068Z",
     "start_time": "2024-05-26T09:20:24.467816Z"
    }
   },
   "outputs": [],
   "source": [
    "class GeminiImageExtractor:\n",
    "    prompt = IMAGE_EXTRACTOR_PROMPT\n",
    "    model_list = ['gemini-1.5-flash-latest',\n",
    "                  'gemini-1.5-pro-latest']\n",
    "    def __init__(self, model_name='gemini-1.5-pro-latest'):\n",
    "        self.api = self._get_api_()\n",
    "        self.chat_model = ChatGoogleGenerativeAI(model=model_name, google_api_key=self.api)\n",
    "        self.text_content = {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": self.prompt\n",
    "        }\n",
    "        \n",
    "    def extract(self, image_path, sleep_time=0):\n",
    "        image_content = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": image_path\n",
    "        }\n",
    "        message = HumanMessage(content=[self.text_content, image_content])\n",
    "        \n",
    "        result = self.chat_model.invoke([message])\n",
    "        \n",
    "        sleep(sleep_time)\n",
    "        return result.content\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_api_(self):\n",
    "        key = os.getenv('GOOGLE_API_KEY')\n",
    "        if not key:\n",
    "            raise InvalidAPIKey\n",
    "        \n",
    "        return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84f08fe06f03df21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.472308Z",
     "start_time": "2024-05-26T09:20:24.470559Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_json(path, items):\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(items, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3579c00654d157",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.474708Z",
     "start_time": "2024-05-26T09:20:24.473479Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_subdirectory(href):\n",
    "    parsed = urlparse(href)\n",
    "    # Check if the scheme and netloc are present\n",
    "    return not (bool(parsed.scheme) and bool(parsed.netloc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f3a3cbdd4cac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.476555Z",
     "start_time": "2024-05-26T09:20:24.475148Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_attachment(url):\n",
    "    # TODO: generalize to various file types\n",
    "    return url.endswith('.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b3bee6db685173",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.478712Z",
     "start_time": "2024-05-26T09:20:24.477106Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_website(soup):\n",
    "    # get the main content\n",
    "    main_content = soup.find('section', {'id': 'sp-main-body'})\n",
    "    \n",
    "    # decompose navigate elements (next or previous page navigators)\n",
    "    navigator_element = main_content.find('ul', {'class': 'pager pagenav'})\n",
    "    if navigator_element:\n",
    "        navigator_element.decompose()\n",
    "        \n",
    "    return main_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8892a21b5e9d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.480679Z",
     "start_time": "2024-05-26T09:20:24.479266Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_web_soup(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29eaee6ce6bf556f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.482627Z",
     "start_time": "2024-05-26T09:20:24.481189Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_url(url):\n",
    "    if is_subdirectory(url):\n",
    "        raise InvalidURL\n",
    "    \n",
    "    parsed_url = urlparse(url)\n",
    "    subdirectory = parsed_url.path\n",
    "    encoded_subdirectory = quote(subdirectory)\n",
    "    encoded_url = f'{parsed_url.scheme}://{parsed_url.netloc}{encoded_subdirectory}'\n",
    "    \n",
    "    return encoded_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f9cb1de5877f91d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.484472Z",
     "start_time": "2024-05-26T09:20:24.483082Z"
    }
   },
   "outputs": [],
   "source": [
    "def website_is_updated(url, hash_value):\n",
    "    soup = get_web_soup(url)\n",
    "    if soup:\n",
    "        main_soup = preprocess_website(soup)\n",
    "        return sha256(main_soup.encode()).hexdigest() != hash_value\n",
    "    \n",
    "    raise Exception('Can not connect to destination URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a092a5ee2f684995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.487156Z",
     "start_time": "2024-05-26T09:20:24.484892Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_links(soup, internal_link=True, external_link=False, attachment=True, start_url=None):    \n",
    "    # get links\n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if attachment and is_attachment(a['href']):\n",
    "            links.add(a['href'])\n",
    "        \n",
    "        else:\n",
    "            is_internal_link = is_subdirectory(a['href'])\n",
    "            if is_internal_link and internal_link:\n",
    "                links.add(encode_url(start_url + a['href']))\n",
    "            elif not is_internal_link and external_link:\n",
    "                links.add(encode_url(a['href']))\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43e32b15fc2dd67a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.489380Z",
     "start_time": "2024-05-26T09:20:24.487692Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_website_image(soup, extractor, start_url=None):\n",
    "    for img in soup.find_all('img', src=True):\n",
    "        url = img['src']\n",
    "        if is_subdirectory(url):\n",
    "            try:\n",
    "                url = start_url + url\n",
    "            except:\n",
    "                raise InvalidURL\n",
    "            \n",
    "        parse_content = extractor.extract(url, sleep_time=5)\n",
    "        if not parse_content.startswith('nothing'):\n",
    "            img.insert_after(parse_content)\n",
    "            \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "224870e356ddfbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.491395Z",
     "start_time": "2024-05-26T09:20:24.489801Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_website_url(soup, start_url):\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if a.string and a.string.strip() != a['href']:\n",
    "            original_url = start_url + a['href'] if is_subdirectory(a['href']) else a['href']\n",
    "            a.string += f' ({original_url})'\n",
    "        \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "761fd8cfb5558b2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.495176Z",
     "start_time": "2024-05-26T09:20:24.493136Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_website(soup, parse_reference=True, parse_image=False, start_url=None):\n",
    "    # kill all script and style elements\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.extract()\n",
    "        \n",
    "    # parse image\n",
    "    if parse_image:\n",
    "        image_extractor = GeminiImageExtractor(model_name='gemini-1.5-flash-latest')\n",
    "        soup = parse_website_image(soup, image_extractor, start_url=start_url)\n",
    "        \n",
    "    # parse references\n",
    "    if parse_reference:\n",
    "        soup = parse_website_url(soup, start_url)\n",
    "        \n",
    "    # parse content\n",
    "    content = soup.get_text()\n",
    "    lines = (line.strip() for line in content.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split('  '))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50fdac7d8db99777",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.497651Z",
     "start_time": "2024-05-26T09:20:24.495666Z"
    }
   },
   "outputs": [],
   "source": [
    "def crawl_website(url, parse_reference=True, parse_image=False, hash=False):\n",
    "    reference_urls = set()\n",
    "    page_content = None\n",
    "    hash_value = None\n",
    "    \n",
    "    soup = get_web_soup(url)\n",
    "    if soup:\n",
    "        main_soup = preprocess_website(soup)\n",
    "        \n",
    "        # hash\n",
    "        if hash:\n",
    "            hash_value = sha256(main_soup.encode()).hexdigest()\n",
    "        \n",
    "        # extract start url\n",
    "        parsed_url = urlparse(url)\n",
    "        start_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        \n",
    "        # get references\n",
    "        reference_urls.update(get_links(main_soup, internal_link=True, external_link=False, attachment=True, start_url=start_url))\n",
    "        \n",
    "        # parse content            \n",
    "        main_content = parse_website(main_soup, parse_reference, parse_image, start_url)\n",
    "        page_content = (main_content, soup.title.string)\n",
    "        \n",
    "    return page_content, reference_urls, hash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "454539368de0e997",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.499342Z",
     "start_time": "2024-05-26T09:20:24.498108Z"
    }
   },
   "outputs": [],
   "source": [
    "def webpage_to_documents(url, page_content, title):\n",
    "    return Document(\n",
    "        page_content=page_content,\n",
    "        metadata={\n",
    "            'source': url,\n",
    "            'title': title\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774ee218e665152",
   "metadata": {},
   "source": [
    "## Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff8361b043ddd",
   "metadata": {},
   "source": [
    "**rule:** There is not any 2 articles have different release year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b01c8ba49bcf471",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.503905Z",
     "start_time": "2024-05-26T09:20:24.499732Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # read crawled file\n",
    "    sitemap_path = 'sitemap.json'\n",
    "    with open(sitemap_path, 'r', encoding='utf-8') as f:\n",
    "        storage_urls = json.load(f)\n",
    "    \n",
    "    start_url = 'https://tuyensinh.hcmus.edu.vn' \n",
    "    news_url = 'https://tuyensinh.hcmus.edu.vn/th%C3%B4ng-tin-tuy%E1%BB%83n-sinh-%C4%91%E1%BA%A1i-h%E1%BB%8Dc'\n",
    "    need2crawl_url = set(storage_urls['base_url']).difference(news_url)\n",
    "    crawled_url = set()\n",
    "    documents = []\n",
    "    \n",
    "    soup = get_web_soup(news_url)\n",
    "    if soup:\n",
    "        main_soup = preprocess_website(soup)\n",
    "        \n",
    "        # check latest article\n",
    "        latest_article_tag = main_soup.find('li')\n",
    "        latest_article_url = latest_article_tag.find('a')['href']\n",
    "        \n",
    "        if latest_article_url not in storage_urls['article_url']:\n",
    "            latest_article_release_date = latest_article_tag.find('span', class_=\"mod-articles-category-date\").text.strip()\n",
    "            random_crawled_url = None\n",
    "            valid_year = datetime.now().year\n",
    "            \n",
    "            if storage_urls['article_url']:\n",
    "                random_crawled_url = list(storage_urls['article_url'])[0]\n",
    "                \n",
    "            # case 2: article is of new year\n",
    "            if random_crawled_url and random_crawled_url['release_date'][-4:] == latest_article_release_date[-4:]:\n",
    "                # delete old crawled article\n",
    "                valid_year = random_crawled_url['release_date'][-4:]\n",
    "\n",
    "            # case 3: update of this year\n",
    "            else:\n",
    "                storage_urls['article_url'].clear()\n",
    "\n",
    "                \n",
    "            # get new articles\n",
    "            new_article_urls = set()\n",
    "            for tag in main_soup.find_all('li'):\n",
    "                release_date = tag.find('span', class_=\"mod-articles-category-date\").text.strip()\n",
    "                url = tag.find('a', class_=\"mod-articles-category-title\")['href']\n",
    "                if url and is_subdirectory(url):\n",
    "                    url = start_url + url\n",
    "                if url not in storage_urls['article_url'] and int(release_date[-4:]) == valid_year:\n",
    "                    new_article_urls.add((encode_url(url), release_date))\n",
    "                    \n",
    "            need2crawl_url.update(new_article_urls)\n",
    "                    \n",
    "            # crawl new article\n",
    "            for url, release_date in new_article_urls:\n",
    "                crawled_url.add(url)\n",
    "                \n",
    "                page_content, references, hash_value = crawl_website(url, hash=True, parse_image=True)\n",
    "                storage_urls['article_url'][url] = {\n",
    "                    'release_date': release_date,\n",
    "                    'hash_value': hash_value, # TODO: whether article need to be hashed?\n",
    "                }\n",
    "                documents.append(webpage_to_documents(url, page_content[0], page_content[1]))\n",
    "                need2crawl_url.update(references)\n",
    "                \n",
    "        # crawl base url\n",
    "        for url in storage_urls['base_url'].keys():\n",
    "            hash_value = storage_urls['base_url'][url].get('hash_value')\n",
    "            if website_is_updated(url, hash_value):\n",
    "                page_content, references, hash_value = crawl_website(url, hash=True, parse_image=True)\n",
    "                storage_urls['base_url'][url] = {\n",
    "                    'hash_value': hash_value,\n",
    "                }\n",
    "                \n",
    "                need2crawl_url.update(references)\n",
    "                documents.append(webpage_to_documents(url, page_content[0], page_content[1]))\n",
    "        \n",
    "        crawled_url.update(storage_urls['base_url'].keys())\n",
    "        \n",
    "        # check if complete all references\n",
    "        # TODO: Handle remaining URLs\n",
    "        need2crawl_url = need2crawl_url.difference(crawled_url)\n",
    "        \n",
    "        # update sitemap file\n",
    "        write_json(sitemap_path, storage_urls)\n",
    "            \n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dc022496418374a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:36:19.354021Z",
     "start_time": "2024-05-26T09:20:24.504492Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5a6dce3b514ec57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:36:19.375126Z",
     "start_time": "2024-05-26T09:36:19.357170Z"
    }
   },
   "outputs": [],
   "source": [
    "json_docs = [json.loads(doc.json(ensure_ascii=False)) for doc in docs]\n",
    "with open('tmp_docs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_docs, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
