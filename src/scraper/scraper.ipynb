{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.464719Z",
     "start_time": "2024-05-26T09:20:24.035080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from hashlib import sha256\n",
    "from urllib.parse import urlparse, quote\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from src.prompts import IMAGE_EXTRACTOR_PROMPT\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "b2c160cc63a54e0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utils",
   "id": "ec2a991871e46c5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.467261Z",
     "start_time": "2024-05-26T09:20:24.465583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InvalidAPIKey(Exception):\n",
    "    message = 'Invalid API Key'\n",
    "    def __init__(self):\n",
    "        super().__init__(self.message)\n",
    "        \n",
    "\n",
    "class InvalidURL(Exception):\n",
    "    message = 'Invalid URL'\n",
    "    def __init__(self):\n",
    "        super().__init__(self.message)"
   ],
   "id": "718b748cc4024ee7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.470068Z",
     "start_time": "2024-05-26T09:20:24.467816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GeminiImageExtractor:\n",
    "    prompt = IMAGE_EXTRACTOR_PROMPT\n",
    "    model_list = ['gemini-1.5-flash-latest',\n",
    "                  'gemini-1.5-pro-latest']\n",
    "    def __init__(self, model_name='gemini-1.5-pro-latest'):\n",
    "        self.api = self._get_api_()\n",
    "        self.chat_model = ChatGoogleGenerativeAI(model=model_name, google_api_key=self.api)\n",
    "        self.text_content = {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": self.prompt\n",
    "        }\n",
    "        \n",
    "    def extract(self, image_path, sleep_time=0):\n",
    "        image_content = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": image_path\n",
    "        }\n",
    "        message = HumanMessage(content=[self.text_content, image_content])\n",
    "        \n",
    "        result = self.chat_model.invoke([message])\n",
    "        \n",
    "        sleep(sleep_time)\n",
    "        return result.content\n",
    "        \n",
    "    def _get_api_(self):\n",
    "        key = os.getenv('GOOGLE_API_KEY')\n",
    "        if not key:\n",
    "            raise InvalidAPIKey\n",
    "        \n",
    "        return key"
   ],
   "id": "5f9ec2fd249c71b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.472308Z",
     "start_time": "2024-05-26T09:20:24.470559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def write_json(path, items):\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(items, file, indent=4)"
   ],
   "id": "84f08fe06f03df21",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.474708Z",
     "start_time": "2024-05-26T09:20:24.473479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_subdirectory(href):\n",
    "    return href.startswith('/')"
   ],
   "id": "2d3579c00654d157",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.476555Z",
     "start_time": "2024-05-26T09:20:24.475148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def is_attachment(url):\n",
    "    # TODO: generalize to various file types\n",
    "    return url.endswith('.pdf')"
   ],
   "id": "63f3a3cbdd4cac6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.478712Z",
     "start_time": "2024-05-26T09:20:24.477106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_website(soup):\n",
    "    # get the main content\n",
    "    main_content = soup.find('section', {'id': 'sp-main-body'})\n",
    "    \n",
    "    # decompose navigate elements (next or previous page navigators)\n",
    "    navigator_element = main_content.find('ul', {'class': 'pager pagenav'})\n",
    "    if navigator_element:\n",
    "        navigator_element.decompose()\n",
    "        \n",
    "    return main_content"
   ],
   "id": "52b3bee6db685173",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.480679Z",
     "start_time": "2024-05-26T09:20:24.479266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_web_soup(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    return None"
   ],
   "id": "8892a21b5e9d3f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.482627Z",
     "start_time": "2024-05-26T09:20:24.481189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_url(url):\n",
    "    if is_subdirectory(url):\n",
    "        raise InvalidURL\n",
    "    \n",
    "    parsed_url = urlparse(url)\n",
    "    subdirectory = parsed_url.path\n",
    "    encoded_subdirectory = quote(subdirectory)\n",
    "    encoded_url = f'{parsed_url.scheme}://{parsed_url.netloc}{encoded_subdirectory}'\n",
    "    \n",
    "    return encoded_url"
   ],
   "id": "29eaee6ce6bf556f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.484472Z",
     "start_time": "2024-05-26T09:20:24.483082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def website_is_updated(url, hash_value):\n",
    "    soup = get_web_soup(url)\n",
    "    if soup:\n",
    "        main_soup = preprocess_website(soup)\n",
    "        return sha256(main_soup.encode()).hexdigest() != hash_value\n",
    "    \n",
    "    raise Exception('Can not connect to destination URL')"
   ],
   "id": "6f9cb1de5877f91d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.487156Z",
     "start_time": "2024-05-26T09:20:24.484892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_links(soup, internal_link=True, external_link=False, attachment=True, start_url=None):    \n",
    "    # get links\n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if attachment and is_attachment(a['href']):\n",
    "            links.add(a['href'])\n",
    "        \n",
    "        else:\n",
    "            is_internal_link = is_subdirectory(a['href'])\n",
    "            if is_internal_link and internal_link:\n",
    "                links.add(encode_url(start_url + a['href']))\n",
    "            elif not is_internal_link and external_link:\n",
    "                links.add(encode_url(a['href']))\n",
    "    \n",
    "    return links"
   ],
   "id": "a092a5ee2f684995",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.489380Z",
     "start_time": "2024-05-26T09:20:24.487692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_website_image(soup, extractor, start_url=None):\n",
    "    for img in soup.find_all('img', src=True):\n",
    "        url = img['src']\n",
    "        if is_subdirectory(url):\n",
    "            try:\n",
    "                url = start_url + url\n",
    "            except:\n",
    "                raise InvalidURL\n",
    "            \n",
    "        parse_content = extractor.extract(url, sleep_time=5)\n",
    "        if not parse_content.startswith('nothing'):\n",
    "            img.insert_after(parse_content)\n",
    "            \n",
    "    return soup"
   ],
   "id": "43e32b15fc2dd67a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.491395Z",
     "start_time": "2024-05-26T09:20:24.489801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_website_url(soup, start_url):\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if a.string and a.string.strip() != a['href']:\n",
    "            original_url = start_url + a['href'] if is_subdirectory(a['href']) else a['href']\n",
    "            a.string += f' ({original_url})'\n",
    "        \n",
    "    return soup"
   ],
   "id": "224870e356ddfbd",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.495176Z",
     "start_time": "2024-05-26T09:20:24.493136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_website(soup, parse_reference=True, parse_image=False, start_url=None):\n",
    "    # kill all script and style elements\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.extract()\n",
    "        \n",
    "    # parse image\n",
    "    if parse_image:\n",
    "        image_extractor = GeminiImageExtractor(model_name='gemini-1.5-flash-latest')\n",
    "        soup = parse_website_image(soup, image_extractor, start_url=start_url)\n",
    "        \n",
    "    # parse references\n",
    "    if parse_reference:\n",
    "        soup = parse_website_url(soup, start_url)\n",
    "        \n",
    "    # parse content\n",
    "    content = soup.get_text()\n",
    "    lines = (line.strip() for line in content.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split('  '))\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return text"
   ],
   "id": "761fd8cfb5558b2c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.497651Z",
     "start_time": "2024-05-26T09:20:24.495666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def crawl_website(url, parse_reference=True, parse_image=False, hash=False):\n",
    "    reference_urls = set()\n",
    "    page_content = None\n",
    "    hash_value = None\n",
    "    \n",
    "    soup = get_web_soup(url)\n",
    "    if soup:\n",
    "        main_soup = preprocess_website(soup)\n",
    "        \n",
    "        # hash\n",
    "        if hash:\n",
    "            hash_value = sha256(main_soup.encode()).hexdigest()\n",
    "        \n",
    "        # extract start url\n",
    "        parsed_url = urlparse(url)\n",
    "        start_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        \n",
    "        # get references\n",
    "        reference_urls.update(get_links(main_soup, internal_link=True, external_link=False, attachment=True, start_url=start_url))\n",
    "        \n",
    "        # parse content            \n",
    "        main_content = parse_website(main_soup, parse_reference, parse_image, start_url)\n",
    "        page_content = (main_content, soup.title.string)\n",
    "        \n",
    "    return page_content, reference_urls, hash_value"
   ],
   "id": "50fdac7d8db99777",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.499342Z",
     "start_time": "2024-05-26T09:20:24.498108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def webpage_to_documents(url, page_content, title):\n",
    "    return Document(\n",
    "        page_content=page_content,\n",
    "        metadata={\n",
    "            'source': url,\n",
    "            'title': title\n",
    "        }\n",
    "    )"
   ],
   "id": "454539368de0e997",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Crawling",
   "id": "e774ee218e665152"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**rule:** There is not any 2 articles have different release year",
   "id": "c2ff8361b043ddd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:20:24.503905Z",
     "start_time": "2024-05-26T09:20:24.499732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # read crawled file\n",
    "    sitemap_path = 'sitemap.json'\n",
    "    with open(sitemap_path, 'r', encoding='utf-8') as f:\n",
    "        storage_urls = json.load(f)\n",
    "    \n",
    "    start_url = 'https://tuyensinh.hcmus.edu.vn' \n",
    "    news_url = 'https://tuyensinh.hcmus.edu.vn/th%C3%B4ng-tin-tuy%E1%BB%83n-sinh-%C4%91%E1%BA%A1i-h%E1%BB%8Dc'\n",
    "    need2crawl_url = set(storage_urls['base_url']).difference(news_url)\n",
    "    crawled_url = set()\n",
    "    documents = []\n",
    "    \n",
    "    soup = get_web_soup(news_url)\n",
    "    if soup:\n",
    "        main_soup = preprocess_website(soup)\n",
    "        \n",
    "        # check latest article\n",
    "        latest_article_tag = main_soup.find('li')\n",
    "        latest_article_url = latest_article_tag.find('a')['href']\n",
    "        \n",
    "        if latest_article_url not in storage_urls['article_url']:\n",
    "            latest_article_release_date = latest_article_tag.find('span', class_=\"mod-articles-category-date\").text.strip()\n",
    "            random_crawled_url = None\n",
    "            valid_year = datetime.now().year\n",
    "            \n",
    "            if storage_urls['article_url']:\n",
    "                random_crawled_url = list(storage_urls['article_url'])[0]\n",
    "                \n",
    "            # case 2: article is of new year\n",
    "            if random_crawled_url and random_crawled_url['release_date'][-4:] == latest_article_release_date[-4:]:\n",
    "                # delete old crawled article\n",
    "                valid_year = random_crawled_url['release_date'][-4:]\n",
    "\n",
    "            # case 3: update of this year\n",
    "            else:\n",
    "                storage_urls['article_url'].clear()\n",
    "\n",
    "                \n",
    "            # get new articles\n",
    "            new_article_urls = set()\n",
    "            for tag in main_soup.find_all('li'):\n",
    "                release_date = tag.find('span', class_=\"mod-articles-category-date\").text.strip()\n",
    "                url = tag.find('a', class_=\"mod-articles-category-title\")['href']\n",
    "                if url and is_subdirectory(url):\n",
    "                    url = start_url + url\n",
    "                if url not in storage_urls['article_url'] and int(release_date[-4:]) == valid_year:\n",
    "                    new_article_urls.add((encode_url(url), release_date))\n",
    "                    \n",
    "            need2crawl_url.update(new_article_urls)\n",
    "                    \n",
    "            # crawl new article\n",
    "            for url, release_date in new_article_urls:\n",
    "                crawled_url.add(url)\n",
    "                \n",
    "                page_content, references, hash_value = crawl_website(url, hash=True, parse_image=True)\n",
    "                storage_urls['article_url'][url] = {\n",
    "                    'release_date': release_date,\n",
    "                    'hash_value': hash_value, # TODO: whether article need to be hashed?\n",
    "                }\n",
    "                documents.append(webpage_to_documents(url, page_content[0], page_content[1]))\n",
    "                need2crawl_url.update(references)\n",
    "                \n",
    "        # crawl base url\n",
    "        for url in storage_urls['base_url'].keys():\n",
    "            hash_value = storage_urls['base_url'][url].get('hash_value')\n",
    "            if website_is_updated(url, hash_value):\n",
    "                page_content, references, hash_value = crawl_website(url, hash=True, parse_image=True)\n",
    "                storage_urls['base_url'][url] = {\n",
    "                    'hash_value': hash_value,\n",
    "                }\n",
    "                \n",
    "                need2crawl_url.update(references)\n",
    "                documents.append(webpage_to_documents(url, page_content[0], page_content[1]))\n",
    "        \n",
    "        crawled_url.update(storage_urls['base_url'].keys())\n",
    "        \n",
    "        # check if complete all references\n",
    "        # TODO: Handle remaining URLs\n",
    "        need2crawl_url = need2crawl_url.difference(crawled_url)\n",
    "        \n",
    "        # update sitemap file\n",
    "        write_json(sitemap_path, storage_urls)\n",
    "            \n",
    "        return documents"
   ],
   "id": "2b01c8ba49bcf471",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:36:19.354021Z",
     "start_time": "2024-05-26T09:20:24.504492Z"
    }
   },
   "cell_type": "code",
   "source": "docs = main()",
   "id": "4dc022496418374a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T09:36:19.375126Z",
     "start_time": "2024-05-26T09:36:19.357170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_docs = [json.loads(doc.json(ensure_ascii=False)) for doc in docs]\n",
    "with open('tmp_docs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_docs, f, ensure_ascii=False, indent=4)"
   ],
   "id": "f5a6dce3b514ec57",
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
